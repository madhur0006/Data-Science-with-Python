{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining using Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Packages Related to Textmining\n",
    "- **textmining1.0:** contains a variety of useful functions for text mining in Python.\n",
    "- **NLTK:** This package can be extremely useful because you have easy access to over 50 corpora and lexical resources\n",
    "- **Tweepy:** to mine Twitter data\n",
    "- **scrappy:**  extract the data you need from websites\n",
    "- **urllib2:** a package for opening URLs\n",
    "- **requests:** library for grabbing data from the internet\n",
    "- **Beautifulsoup:** library for parsing HTML data\n",
    "- **re:**  grep(), grepl(), regexpr(), gregexpr(), sub(), gsub(), and strsplit() are helpful functions\n",
    "- **wordcloud:** to visualize the wordcloud\n",
    "- **Textblob:** to used for text processing (nlp- lowel events)\n",
    "- **sklearn:** to used for preprocessing, modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing - What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "##### TF-IDF Vectors as features\n",
    "- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "    - a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
    "    - b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
    "    - c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n",
    "\n",
    "##### Text / NLP based features\n",
    "- Word Count of the documents – total number of words in the documents\n",
    "- Character Count of the documents – total number of characters in the documents\n",
    "- Average Word Density of the documents – average length of the words used in the documents\n",
    "- Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "- Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "- Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
    "- Frequency distribution of Part of Speech Tags:\n",
    "    - Noun Count\n",
    "    - Verb Count\n",
    "    - Adjective Count\n",
    "    - Adverb Count\n",
    "    - ronoun Count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "- Naive Bayes Classifier\n",
    "- Linear Classifier\n",
    "- Support Vector Machine\n",
    "- KNN\n",
    "- Bagging Models\n",
    "- Boosting Models\n",
    "- Shallow Neural Networks\n",
    "- Deep Neural Networks\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Long Short Term Modelr (LSTM)\n",
    "    - Gated Recurrent Unit (GRU)\n",
    "    - Bidirectional RNN\n",
    "    - Recurrent Convolutional Neural Network (RCNN)\n",
    "    - Other Variants of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import required packages\n",
    "#basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#stats\n",
    "#from scipy.misc import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from PIL import Image\n",
    "#import matplotlib_venn as venn\n",
    "\n",
    "#nlp\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "\n",
    "#FeatureEngineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import  textblob\n",
    "#import xgboost\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from textblob import Word\n",
    "\n",
    "#settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "yelp = pd.read_csv('yelp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp=yelp[['review_id', 'stars', 'text', 'cool', 'useful', 'funny']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text  cool  useful  funny  \n",
       "0  My wife took me here on my birthday for breakf...     2       5      0  \n",
       "1  I have no idea why some people give bad review...     0       0      0  \n",
       "2  love the gyro plate. Rice is so good and I als...     0       1      0  \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...     1       2      0  \n",
       "4  General Manager Scott Petello is a good egg!!!...     0       0      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['count_sent']=df[\"text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
    "\n",
    "#Word count in each comment:\n",
    "df['count_word']=df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count\n",
    "df['count_unique_word']=df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count\n",
    "df['count_letters']=df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#Word density\n",
    "\n",
    "df['word_density'] = df['count_letters'] / (df['count_word']+1)\n",
    "\n",
    "#punctuation count\n",
    "df[\"count_punctuations\"] =df[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_upper\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#upper case words count\n",
    "df[\"count_words_lower\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
    "\n",
    "#title case words count\n",
    "df[\"count_words_title\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords\n",
    "df[\"count_stopwords\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "#Average length of the words\n",
    "df[\"mean_word_len\"] = df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "#Number of numeric\n",
    "df['numeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "#Number of alphanumeric\n",
    "df['alphanumeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalnum()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['alphabetetics'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalpha()]))\n",
    "\n",
    "#Number of alphabetics\n",
    "df['Spaces'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isspace()]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_ends_with_et'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.endswith('et')]))\n",
    "\n",
    "#Number of Words ends with\n",
    "df['words_start_with_no'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.startswith('no')]))\n",
    "\n",
    "# Count the occurences of all words\n",
    "df['wordcounts'] = df['text'].apply(lambda x :dict([ [t, x.split().count(t)] for t in set(x.split()) ]))\n",
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "df['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "df['verb_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "df['adj_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "df['adv_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "df['pron_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'pron')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.402469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.608646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars  \\\n",
       "0  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text  cool  useful  funny  \\\n",
       "0  My wife took me here on my birthday for breakf...     2       5      0   \n",
       "1  I have no idea why some people give bad review...     0       0      0   \n",
       "2  love the gyro plate. Rice is so good and I als...     0       1      0   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...     1       2      0   \n",
       "4  General Manager Scott Petello is a good egg!!!...     0       0      0   \n",
       "\n",
       "   sentiment  \n",
       "0   0.402469  \n",
       "1   0.229773  \n",
       "2   0.566667  \n",
       "3   0.608646  \n",
       "4   0.468125  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500,)\n",
      "(2500,)\n",
      "(7500,)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "#yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp.text\n",
    "y = yelp.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = 'Analytixlabs is from bagnalore, it has offices in Gurgoan, KL. It is staarted in 4Years back'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Abbrevations and Words correction\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'analytixlabs is from bagnalore it has offices in gurgoan kl it is staarted in years back'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"don't\", 'yourself', 'needn', 'myself', 'been', 'he', 'theirs', \"haven't\", 'haven', 'do', 'out', 'mightn', 'weren', 'from', 'some', \"you're\", 'be', 'himself', 'with', 'by', 'can', 'm', 'o', 'being', 'hasn', \"hasn't\", \"didn't\", 'to', 'more', 's', 'couldn', 'its', \"isn't\", 'themselves', 'each', \"wasn't\", 'your', 'their', 'an', 'most', 'only', 'won', \"you'd\", 'then', 'same', 'of', 'just', 'don', 'under', 'again', 'y', \"mustn't\", \"you'll\", 'on', 'such', \"won't\", 're', \"hadn't\", \"shan't\", 'ma', 'herself', \"needn't\", 'does', 'there', 'where', 'if', \"shouldn't\", 'she', 'you', 'will', 'through', 'mustn', \"weren't\", 'no', 'shan', 'have', 'that', 'few', 't', 'as', 'down', 'has', 'isn', 'didn', 'ourselves', 'is', 'here', 'while', 'her', 'the', 'at', \"you've\", 'doing', 'should', 'up', 'during', 'having', 'over', 'ours', 'hers', 'doesn', 'yourselves', 'when', 'but', 'very', 'they', 'once', 'into', 'we', 'until', 'and', 'own', \"couldn't\", 'me', 'am', 'a', 'yours', \"she's\", \"it's\", 'for', 'who', 've', 'both', 'further', 'how', 'which', 'what', 'had', 'not', 'aren', \"mightn't\", 'were', 'll', 'other', 'these', 'or', 'below', 'why', \"wouldn't\", 'them', 'now', 'those', 'our', 'before', 'itself', 'was', 'shouldn', 'too', 'wouldn', 'i', 'all', 'whom', 'so', 'off', 'this', \"should've\", 'ain', \"aren't\", 'his', 'him', 'between', 'against', 'are', 'about', 'after', 'any', 'my', 'hadn', \"that'll\", 'in', 'it', 'because', 'above', \"doesn't\", 'did', 'wasn', 'than', 'd', 'nor'}\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer_func = nltk.stem.snowball.SnowballStemmer(\"english\").stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s= 'Analytics is really doing good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realli'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_func('really')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analytics', 'is', 'really', 'doing', 'good']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def pre_process(text):\n",
    "    #text = text.str.replace('/','')\n",
    "    #text = text.apply(lambda x: re.sub(\"  \",\" \", x))\n",
    "    #text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,']\", \"\", text)\n",
    "    #text = re.sub(r'[0-9]+', '', text)\n",
    "    #text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n",
    "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    #text = text.apply(lambda x: str(TextBlob(x).correct()))\n",
    "    #text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split()))\n",
    "    #text = text.apply(lambda x: \" \".join(stemmer_func(word) for word in x.split()))\n",
    "    #text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    #text = text.apply(lambda x: \" \".join(word for word, pos in pos_tag(x.split()) if pos not in ['NN','NNS','NNP','NNPS']))\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: clean_text(x))\n",
    "X_test = X_test.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=pre_process(X_train)\n",
    "X_test=pre_process(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 1 ), min_df=5, encoding='latin-1' , max_features=800)\n",
    "xtrain_count = count_vect.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm=xtrain_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'ambiance',\n",
       " 'amount',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'appetizer',\n",
       " 'appetizers',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arizona',\n",
       " 'around',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'ate',\n",
       " 'atmosphere',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'authentic',\n",
       " 'available',\n",
       " 'average',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'az',\n",
       " 'back',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'bar',\n",
       " 'bartender',\n",
       " 'based',\n",
       " 'bbq',\n",
       " 'beans',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'beef',\n",
       " 'beer',\n",
       " 'beers',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bite',\n",
       " 'black',\n",
       " 'bland',\n",
       " 'blue',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'bread',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'brought',\n",
       " 'bucks',\n",
       " 'buffet',\n",
       " 'burger',\n",
       " 'burgers',\n",
       " 'burrito',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'case',\n",
       " 'casual',\n",
       " 'center',\n",
       " 'chain',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'charge',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'chef',\n",
       " 'chicken',\n",
       " 'chili',\n",
       " 'chinese',\n",
       " 'chips',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choices',\n",
       " 'choose',\n",
       " 'city',\n",
       " 'clean',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'club',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'company',\n",
       " 'completely',\n",
       " 'cook',\n",
       " 'cooked',\n",
       " 'cool',\n",
       " 'corn',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'counter',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'crab',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'crispy',\n",
       " 'crowd',\n",
       " 'crowded',\n",
       " 'crust',\n",
       " 'cup',\n",
       " 'curry',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'dark',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decor',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'desert',\n",
       " 'dessert',\n",
       " 'didnt',\n",
       " 'different',\n",
       " 'dining',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'dirty',\n",
       " 'disappointed',\n",
       " 'dish',\n",
       " 'dishes',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'downtown',\n",
       " 'dr',\n",
       " 'dressing',\n",
       " 'drink',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'dry',\n",
       " 'early',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'egg',\n",
       " 'eggs',\n",
       " 'either',\n",
       " 'else',\n",
       " 'employees',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'entree',\n",
       " 'entrees',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'felt',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'five',\n",
       " 'flavor',\n",
       " 'flavorful',\n",
       " 'flavors',\n",
       " 'food',\n",
       " 'foods',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'free',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'fries',\n",
       " 'front',\n",
       " 'frozen',\n",
       " 'fruit',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'game',\n",
       " 'garlic',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'grab',\n",
       " 'greasy',\n",
       " 'great',\n",
       " 'green',\n",
       " 'grill',\n",
       " 'grilled',\n",
       " 'group',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'havent',\n",
       " 'head',\n",
       " 'healthy',\n",
       " 'heard',\n",
       " 'help',\n",
       " 'helpful',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'hit',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'hope',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'however',\n",
       " 'huge',\n",
       " 'hungry',\n",
       " 'husband',\n",
       " 'ice',\n",
       " 'iced',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'impressed',\n",
       " 'including',\n",
       " 'ingredients',\n",
       " 'inside',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'isnt',\n",
       " 'italian',\n",
       " 'item',\n",
       " 'items',\n",
       " 'ive',\n",
       " 'job',\n",
       " 'joint',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'kids',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'la',\n",
       " 'lady',\n",
       " 'large',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'lettuce',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'line',\n",
       " 'list',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'local',\n",
       " 'located',\n",
       " 'location',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'loud',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'low',\n",
       " 'lunch',\n",
       " 'mac',\n",
       " 'made',\n",
       " 'main',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'mall',\n",
       " 'man',\n",
       " 'manager',\n",
       " 'many',\n",
       " 'market',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'meal',\n",
       " 'meals',\n",
       " 'mean',\n",
       " 'meat',\n",
       " 'menu',\n",
       " 'mexican',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mix',\n",
       " 'mixed',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'months',\n",
       " 'morning',\n",
       " 'mouth',\n",
       " 'moved',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'music',\n",
       " 'must',\n",
       " 'name',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'neighborhood',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'noodles',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'noticed',\n",
       " 'number',\n",
       " 'offer',\n",
       " 'offered',\n",
       " 'office',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'onion',\n",
       " 'onions',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'options',\n",
       " 'orange',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'others',\n",
       " 'outdoor',\n",
       " 'outside',\n",
       " 'overall',\n",
       " 'overpriced',\n",
       " 'owner',\n",
       " 'owners',\n",
       " 'packed',\n",
       " 'park',\n",
       " 'parking',\n",
       " 'part',\n",
       " 'party',\n",
       " 'past',\n",
       " 'pasta',\n",
       " 'patio',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'perhaps',\n",
       " 'person',\n",
       " 'pho',\n",
       " 'phoenix',\n",
       " 'phone',\n",
       " 'pick',\n",
       " 'pie',\n",
       " 'piece',\n",
       " 'pieces',\n",
       " 'pita',\n",
       " 'pizza',\n",
       " 'place',\n",
       " 'places',\n",
       " 'plate',\n",
       " 'plates',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'pleasant',\n",
       " 'please',\n",
       " 'plenty',\n",
       " 'plus',\n",
       " 'pm',\n",
       " 'point',\n",
       " 'pool',\n",
       " 'pork',\n",
       " 'portion',\n",
       " 'portions',\n",
       " 'potato',\n",
       " 'potatoes',\n",
       " 'prefer',\n",
       " 'prepared',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'priced',\n",
       " 'prices',\n",
       " 'pricey',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'put',\n",
       " 'quality',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'reasonable',\n",
       " 'received',\n",
       " 'recently',\n",
       " 'recommend',\n",
       " 'recommended',\n",
       " 'red',\n",
       " 'regular',\n",
       " 'remember',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'restaurants',\n",
       " 'return',\n",
       " 'review',\n",
       " 'reviews',\n",
       " 'ribs',\n",
       " 'rice',\n",
       " 'right',\n",
       " 'roasted',\n",
       " 'roll',\n",
       " 'rolls',\n",
       " 'room',\n",
       " 'run',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'salads',\n",
       " 'salmon',\n",
       " 'salsa',\n",
       " 'salt',\n",
       " 'sandwich',\n",
       " 'sandwiches',\n",
       " 'sat',\n",
       " 'saturday',\n",
       " 'sauce',\n",
       " 'sauces',\n",
       " 'sausage',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'scottsdale',\n",
       " 'seafood',\n",
       " 'seat',\n",
       " 'seated',\n",
       " 'seating',\n",
       " 'seats',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'selection',\n",
       " 'seriously',\n",
       " 'serve',\n",
       " 'served',\n",
       " 'server',\n",
       " 'servers',\n",
       " 'service',\n",
       " 'serving',\n",
       " 'set',\n",
       " 'several',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'shot',\n",
       " 'show',\n",
       " 'shrimp',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'sign',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sit',\n",
       " 'sitting',\n",
       " 'size',\n",
       " 'slices',\n",
       " 'slightly',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'soft',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'soup',\n",
       " 'space',\n",
       " 'special',\n",
       " 'specials',\n",
       " 'spend',\n",
       " 'spent',\n",
       " 'spicy',\n",
       " 'spinach',\n",
       " 'split',\n",
       " 'sports',\n",
       " 'spot',\n",
       " 'spring',\n",
       " 'staff',\n",
       " 'stand',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'start',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'stayed',\n",
       " 'steak',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'stores',\n",
       " 'street',\n",
       " 'strip',\n",
       " 'stuff',\n",
       " 'stuffed',\n",
       " 'style',\n",
       " 'summer',\n",
       " 'sunday',\n",
       " 'super',\n",
       " 'sure',\n",
       " 'surprised',\n",
       " 'sushi',\n",
       " 'sweet',\n",
       " 'table',\n",
       " 'tables',\n",
       " 'taco',\n",
       " 'tacos',\n",
       " 'take',\n",
       " 'takes',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'taste',\n",
       " 'tasted',\n",
       " 'tastes',\n",
       " 'tasting',\n",
       " 'tasty',\n",
       " 'tea',\n",
       " 'tell',\n",
       " 'tempe',\n",
       " 'tender',\n",
       " 'th',\n",
       " 'thai',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'theres',\n",
       " 'theyre',\n",
       " 'thin',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tiny',\n",
       " 'tip',\n",
       " 'toast',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomato',\n",
       " 'tomatoes',\n",
       " 'tons',\n",
       " 'took',\n",
       " 'top',\n",
       " 'toppings',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'town',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tuna',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'type',\n",
       " 'typical',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unique',\n",
       " 'unless',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'usually',\n",
       " 'valley',\n",
       " 'value',\n",
       " 'variety',\n",
       " 'vegetarian',\n",
       " 'veggie',\n",
       " 'veggies',\n",
       " 'vibe',\n",
       " 'view',\n",
       " 'visit',\n",
       " 'w',\n",
       " 'wait',\n",
       " 'waited',\n",
       " 'waiter',\n",
       " 'waiting',\n",
       " 'waitress',\n",
       " 'walk',\n",
       " 'walked',\n",
       " 'walking',\n",
       " 'wall',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'warm',\n",
       " 'wasnt',\n",
       " 'watch',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'well',\n",
       " 'went',\n",
       " 'werent',\n",
       " 'weve',\n",
       " 'whatever',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wife',\n",
       " 'wine',\n",
       " 'wings',\n",
       " 'wish',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'work',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wouldnt',\n",
       " 'wow',\n",
       " 'wrong',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yelp',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'youll',\n",
       " 'youre',\n",
       " 'youve',\n",
       " 'yum',\n",
       " 'yummy']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm1=pd.DataFrame(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm1.columns=count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>almost</th>\n",
       "      <th>along</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  across  actually  add  added  afternoon  ago  almost  \\\n",
       "0     0           0       0         0    0      0          0    0       0   \n",
       "1     0           0       0         0    0      0          0    0       0   \n",
       "2     0           2       0         1    0      0          0    0       0   \n",
       "3     0           1       0         0    0      0          0    0       0   \n",
       "4     0           0       0         0    1      0          0    0       0   \n",
       "\n",
       "   along  ...    year  years  yelp  yes  yet  youll  youre  youve  yum  yummy  \n",
       "0      0  ...       0      0     0    0    0      0      0      0    0      0  \n",
       "1      0  ...       0      0     0    0    0      0      0      0    0      0  \n",
       "2      0  ...       0      0     0    0    0      0      0      0    0      0  \n",
       "3      0  ...       0      0     0    1    0      0      0      0    0      0  \n",
       "4      1  ...       0      0     0    0    1      0      0      0    0      0  \n",
       "\n",
       "[5 rows x 800 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 1 ), min_df=5, encoding='latin-1' , max_features=800)\n",
    "xtrain_count = count_vect.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(xtrain_count)\n",
    "\n",
    "#Test\n",
    "#count_vect = CountVectorizer()\n",
    "xtest_count = count_vect.transform(X_test)\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "X_test_tfidf = tfidf_transformer.transform(xtest_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtm2=pd.DataFrame(X_train_tfidf.toarray(), columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>added</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>ago</th>\n",
       "      <th>almost</th>\n",
       "      <th>along</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>youll</th>\n",
       "      <th>youre</th>\n",
       "      <th>youve</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092854</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424834</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>0.140875</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.259271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.266306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.188855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        able  absolutely  across  actually       add     added  afternoon  \\\n",
       "0   0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "1   0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "2   0.000000    0.243022     0.0  0.100958  0.000000  0.000000   0.000000   \n",
       "3   0.000000    0.279355     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "4   0.000000    0.000000     0.0  0.000000  0.126434  0.000000   0.000000   \n",
       "5   0.000000    0.000000     0.0  0.068286  0.000000  0.000000   0.092854   \n",
       "6   0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "7   0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "8   0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "9   0.000000    0.000000     0.0  0.227758  0.000000  0.000000   0.000000   \n",
       "10  0.000000    0.000000     0.0  0.203209  0.000000  0.000000   0.000000   \n",
       "11  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "12  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "13  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "14  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "15  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "16  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "17  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "18  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "19  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "20  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "21  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "22  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "23  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "24  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "25  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "26  0.000000    0.125978     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "27  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "28  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "29  0.000000    0.000000     0.0  0.000000  0.000000  0.142691   0.140875   \n",
       "..       ...         ...     ...       ...       ...       ...        ...   \n",
       "70  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "71  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "72  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "73  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "74  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "75  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "76  0.259271    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "77  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "78  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "79  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "80  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "81  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "82  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "83  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "84  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "85  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "86  0.188855    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "87  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "88  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "89  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "90  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "91  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "92  0.000000    0.000000     0.0  0.064749  0.000000  0.000000   0.000000   \n",
       "93  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "94  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "95  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "96  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "97  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "98  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "99  0.000000    0.000000     0.0  0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "       ago    almost     along    ...         year  years      yelp       yes  \\\n",
       "0   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "1   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "2   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "3   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.254748   \n",
       "4   0.0000  0.000000  0.126715    ...     0.000000    0.0  0.000000  0.000000   \n",
       "5   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "6   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "7   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "8   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "9   0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "10  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "11  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "12  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "13  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "14  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "15  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "16  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "17  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "18  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "19  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "20  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "21  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "22  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "23  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "24  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "25  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "26  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "27  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "28  0.0000  0.000000  0.000000    ...     0.104826    0.0  0.000000  0.000000   \n",
       "29  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "..     ...       ...       ...    ...          ...    ...       ...       ...   \n",
       "70  0.1888  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "71  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "72  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "73  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "74  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "75  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "76  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "77  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "78  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "79  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "80  0.0000  0.000000  0.112676    ...     0.000000    0.0  0.000000  0.000000   \n",
       "81  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "82  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "83  0.0000  0.000000  0.000000    ...     0.153880    0.0  0.000000  0.000000   \n",
       "84  0.0000  0.266306  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "85  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "86  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "87  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "88  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "89  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "90  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "91  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "92  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.076393  0.000000   \n",
       "93  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "94  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "95  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "96  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "97  0.0000  0.000000  0.000000    ...     0.142237    0.0  0.000000  0.000000   \n",
       "98  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "99  0.0000  0.000000  0.000000    ...     0.000000    0.0  0.000000  0.000000   \n",
       "\n",
       "         yet  youll     youre  youve       yum     yummy  \n",
       "0   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "1   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "2   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "3   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "4   0.114775    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "5   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "6   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "7   0.000000    0.0  0.000000    0.0  0.424834  0.000000  \n",
       "8   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "9   0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "10  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "11  0.000000    0.0  0.175633    0.0  0.000000  0.000000  \n",
       "12  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "13  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "14  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "15  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "16  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "17  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "18  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "19  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "20  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "21  0.000000    0.0  0.116464    0.0  0.000000  0.000000  \n",
       "22  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "23  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "24  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "25  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "26  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "27  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "28  0.100825    0.0  0.086203    0.0  0.000000  0.000000  \n",
       "29  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "..       ...    ...       ...    ...       ...       ...  \n",
       "70  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "71  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "72  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "73  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "74  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "75  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "76  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "77  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "78  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "79  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "80  0.000000    0.0  0.000000    0.0  0.000000  0.104622  \n",
       "81  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "82  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "83  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "84  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "85  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "86  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "87  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "88  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "89  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "90  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "91  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "92  0.000000    0.0  0.125073    0.0  0.000000  0.000000  \n",
       "93  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "94  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "95  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "96  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "97  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "98  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "99  0.000000    0.0  0.000000    0.0  0.000000  0.000000  \n",
       "\n",
       "[100 rows x 800 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), max_features=800)\n",
    "tfidf_vect_ngram.fit(df['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(1,2), max_features=800)\n",
    "tfidf_vect_ngram_chars.fit(df['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic Models as features\n",
    "\n",
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(X_train_tfidf)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['average sports charge money super low options given crowded prices area center go except dining end desert outdoor clean cant rather leave watch business cheap better like though long drinks one head come really small want sure bar find dont make city employees literally shopping side mac truly also always',\n",
       " 'yum weeks awesome delicious great beer say bucks surprised food place good year want worth service need burger pizza try kind think menu corn chicken id mind find low room really excellent different got large money parking stand summer price go wasnt stayed cute pool quick excited dont lunch side',\n",
       " 'place great good like food get go one really time bar dont love nice always people service coffee ive would back pretty im night staff better well little happy area also fun friendly beer much drinks hour even location know think going room want never see around way youre new',\n",
       " 'great food service love good best excellent place atmosphere amazing awesome ever always breakfast yummy town must wow friendly pizza customer bar drive ive return go expensive staff enjoyed restaurant top super recommend beautiful crust fun never owner new fresh pool experience pita garlic bread get chicken could week worth',\n",
       " 'others real home good doesnt wait however taste back pork different w far parking made never shrimp much owners thanks charge beat awesome favorite fruit keep sandwich rice cheap seating prices near ill already flavors absolutely tempe food weekend patio chocolate etc finally yes cant guess ever decent even place',\n",
       " 'phoenix crowded owners best park offer great love space place excellent never clean table amazing lot fun back prices us try friendly including tell party one veggie chips door hour go check trip anywhere friends four good cost show wife service nice help easy hot casual see pm least atmosphere',\n",
       " 'closed priced salad eat star good food sandwich great place long definitely time egg also park close ago wont way oh location visit stopped recently grab well center plate trip cup ordered lunch service next watching th wife side scottsdale waitress delicious dishes ever lots one fresh wonderful says atmosphere',\n",
       " 'great food good friendly fast staff service clean delicious pizza ice mexican sports prices best place selection fresh cream love always nice eat lunch super beer awesome amazing fries salad healthy valley price burgers burger chicken favorite decent quick tasty definitely also salads specials sandwiches restaurant beat yummy bad salsa',\n",
       " 'art see parking makes pretty crazy salmon homemade werent unless sunday egg start city able options hand place spent time free spot really pork light tried end one leave lunch atmosphere overall walk times like hours fact doesnt couple must half last menu try enjoyed want high two wanted make',\n",
       " 'attentive amazing delicious nice service like great food place opened soup ago room sauce wall beef free sit pieces type eating neighborhood mall hands seat sausage prefer bbq must perfect cute husband things literally birthday amount beers choices cream last today think able mac unique rather arrived water parking chicken',\n",
       " 'store great time service customer dr get staff one back told never go recommend would car always care work job call friendly place experience helpful going need office phone hair even like new shop help really home years could went love good us got nice highly ever well needed found',\n",
       " 'fabulous wish location bacon pie tuna prefer perfect buy visit place neighborhood sandwich meals love great spot miss pork made simple sandwiches getting la together definitely drinks top nice local go remember black could market enjoyed entree arizona lunch server find night rice crab wine prices good come food dinner',\n",
       " 'tea iced like variety chocolate coffee yum sandwich authentic great shop mall grab place one seems best sandwiches yummy taste food better hot ever side mouth home way choices find area last thats back prices probably anyway course right times valley fast tasty ive available bite little nice restaurant delicious',\n",
       " 'sushi thai roll rolls spicy chinese tuna noodles curry reasonable crab shrimp soup fresh beef salmon dishes tea egg prices craving good chicken food rice great bland restaurant fried delicious white excellent love w dish salad asian place sports iced price best order fish menu try taste quite dinner like',\n",
       " 'hours open dirty coffee everything large dinner place get bad says drinks flavorful morning arent want always big watch either think pretty one decor door selection great bar looking stopped clean mix decent breakfast chocolate single trying seating go like isnt area dessert free az places fantastic feel pizza people',\n",
       " 'dog dogs received foods whole dr onion hot love best store highly recommended value great fantastic hair park shopping cheese selection main happy visit recommend makes around always place go shop center single service one youve got leave get ok like must walk location ever wine feel food packed delicious',\n",
       " 'good food place great like chicken ordered one really salad service get go time cheese would back pizza menu also delicious restaurant sauce best try little love lunch order fresh ive us im dont burger sandwich got well meal nice pretty even eat fries always tasty didnt bread went came',\n",
       " 'oil soon ready friendly back look tomato forward staff chicken food attentive good impressed service without party perfectly grilled definitely delicious shared cheese bad first order ordered counter time breakfast crispy lunch go stores specials coffee busy ive quick even fries going tomatoes went ever behind still egg tasty seating',\n",
       " 'steak chicken city salmon cooked later minutes bartender bill food curry went menu bland friday came best glad took brought fish meal lunch ordered else friend nice good get would didnt tasted sides meals french place seemed could mine something ate fresh sauce though spicy simple sorry bit potatoes outside',\n",
       " 'horrible wont ok except bread service food absolutely wait dry would store read ordering going got counter around back best eggs ambiance anyone spent beef end ended nothing wife line highly hours money fish selection meat loved thought recommend cool go staff still amazing dogs new total well time tiny']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the topic models\n",
    "n_top_words = 50\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "\n",
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency_words_wo_stop= {}\n",
    "for data in yelp['text']:\n",
    "    tokens = nltk.wordpunct_tokenize(data.lower())\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stop:\n",
    "            if token in frequency_words_wo_stop:\n",
    "                count = frequency_words_wo_stop[token]\n",
    "                count = count + 1\n",
    "                frequency_words_wo_stop[token] = count\n",
    "            else:\n",
    "                frequency_words_wo_stop[token] = 1\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wife': 365,\n",
       " 'took': 759,\n",
       " 'birthday': 202,\n",
       " 'breakfast': 737,\n",
       " 'excellent': 724,\n",
       " '.': 75581,\n",
       " 'weather': 92,\n",
       " 'perfect': 649,\n",
       " 'made': 1334,\n",
       " 'sitting': 276,\n",
       " 'outside': 594,\n",
       " 'overlooking': 12,\n",
       " 'grounds': 38,\n",
       " 'absolute': 57,\n",
       " 'pleasure': 55,\n",
       " 'waitress': 426,\n",
       " 'food': 6184,\n",
       " 'arrived': 287,\n",
       " 'quickly': 266,\n",
       " 'semi': 37,\n",
       " '-': 9550,\n",
       " 'busy': 498,\n",
       " 'saturday': 301,\n",
       " 'morning': 323,\n",
       " 'looked': 464,\n",
       " 'like': 5041,\n",
       " 'place': 6662,\n",
       " 'fills': 16,\n",
       " 'pretty': 1812,\n",
       " 'earlier': 73,\n",
       " 'get': 3819,\n",
       " 'better': 1541,\n",
       " 'favor': 41,\n",
       " 'bloody': 54,\n",
       " 'mary': 46,\n",
       " 'phenomenal': 59,\n",
       " 'simply': 177,\n",
       " 'best': 1952,\n",
       " \"'\": 27668,\n",
       " 'ever': 1081,\n",
       " 'sure': 1149,\n",
       " 'use': 485,\n",
       " 'ingredients': 276,\n",
       " 'garden': 89,\n",
       " 'blend': 39,\n",
       " 'fresh': 1222,\n",
       " 'order': 1589,\n",
       " 'amazing': 1060,\n",
       " 'everything': 1066,\n",
       " 'menu': 1678,\n",
       " 'looks': 293,\n",
       " ',': 53283,\n",
       " 'white': 382,\n",
       " 'truffle': 42,\n",
       " 'scrambled': 21,\n",
       " 'eggs': 239,\n",
       " 'vegetable': 71,\n",
       " 'skillet': 28,\n",
       " 'tasty': 863,\n",
       " 'delicious': 1339,\n",
       " 'came': 1309,\n",
       " '2': 1431,\n",
       " 'pieces': 200,\n",
       " 'griddled': 3,\n",
       " 'bread': 788,\n",
       " 'absolutely': 318,\n",
       " 'meal': 1050,\n",
       " 'complete': 104,\n",
       " '\"': 4517,\n",
       " 'toast': 183,\n",
       " 'anyway': 256,\n",
       " 'wait': 1128,\n",
       " 'go': 3571,\n",
       " 'back': 2889,\n",
       " '!': 8729,\n",
       " 'idea': 210,\n",
       " 'people': 1675,\n",
       " 'give': 1053,\n",
       " 'bad': 1031,\n",
       " 'reviews': 468,\n",
       " 'goes': 176,\n",
       " 'show': 269,\n",
       " 'please': 244,\n",
       " 'everyone': 563,\n",
       " 'probably': 686,\n",
       " 'griping': 2,\n",
       " 'something': 1104,\n",
       " 'fault': 59,\n",
       " '...': 4224,\n",
       " 'many': 1024,\n",
       " 'case': 181,\n",
       " 'friend': 713,\n",
       " '5': 1257,\n",
       " ':': 2034,\n",
       " '50': 301,\n",
       " 'pm': 98,\n",
       " 'past': 295,\n",
       " 'sunday': 269,\n",
       " 'crowded': 199,\n",
       " 'thought': 673,\n",
       " 'evening': 246,\n",
       " 'would': 3105,\n",
       " 'forever': 104,\n",
       " 'seat': 154,\n",
       " 'said': 1038,\n",
       " 'seated': 272,\n",
       " 'girl': 234,\n",
       " 'comes': 413,\n",
       " 'seating': 326,\n",
       " 'someone': 437,\n",
       " 'else': 577,\n",
       " '52': 6,\n",
       " 'waiter': 345,\n",
       " 'got': 1831,\n",
       " 'drink': 842,\n",
       " 'orders': 155,\n",
       " 'pleasant': 183,\n",
       " 'host': 64,\n",
       " 'us': 1907,\n",
       " 'server': 664,\n",
       " 'prices': 905,\n",
       " 'good': 6801,\n",
       " 'well': 2210,\n",
       " 'placed': 76,\n",
       " 'decided': 559,\n",
       " 'wanted': 676,\n",
       " '6': 451,\n",
       " '02': 4,\n",
       " 'shared': 159,\n",
       " 'baked': 148,\n",
       " 'spaghetti': 47,\n",
       " 'calzone': 32,\n",
       " 'small': 942,\n",
       " 'beef': 624,\n",
       " 'pizza': 1372,\n",
       " 'try': 1721,\n",
       " 'huge': 715,\n",
       " 'smallest': 10,\n",
       " 'one': 4106,\n",
       " '(': 5652,\n",
       " 'personal': 141,\n",
       " ')': 2645,\n",
       " '11': 159,\n",
       " 'awesome': 827,\n",
       " 'liked': 437,\n",
       " 'sweetish': 2,\n",
       " 'sauce': 1222,\n",
       " 'box': 165,\n",
       " 'part': 475,\n",
       " 'take': 1211,\n",
       " 'home': 902,\n",
       " 'door': 430,\n",
       " '42': 8,\n",
       " 'great': 5127,\n",
       " 'reviewers': 50,\n",
       " 'things': 833,\n",
       " 'serious': 80,\n",
       " 'issues': 87,\n",
       " 'love': 2250,\n",
       " 'gyro': 94,\n",
       " 'plate': 502,\n",
       " 'rice': 646,\n",
       " 'also': 2516,\n",
       " 'dig': 52,\n",
       " 'candy': 82,\n",
       " 'selection': 726,\n",
       " ':)': 337,\n",
       " 'rosie': 6,\n",
       " 'dakota': 2,\n",
       " 'chaparral': 8,\n",
       " 'dog': 335,\n",
       " 'park': 279,\n",
       " '!!!': 438,\n",
       " 'convenient': 96,\n",
       " 'surrounded': 20,\n",
       " 'lot': 985,\n",
       " 'paths': 10,\n",
       " 'desert': 204,\n",
       " 'xeriscape': 1,\n",
       " 'baseball': 50,\n",
       " 'fields': 22,\n",
       " 'ballparks': 1,\n",
       " 'lake': 34,\n",
       " 'ducks': 11,\n",
       " 'scottsdale': 673,\n",
       " 'rec': 5,\n",
       " 'dept': 13,\n",
       " 'wonderful': 473,\n",
       " 'job': 344,\n",
       " 'keeping': 53,\n",
       " 'clean': 746,\n",
       " 'shaded': 11,\n",
       " 'find': 1068,\n",
       " 'trash': 38,\n",
       " 'cans': 22,\n",
       " 'poopy': 1,\n",
       " 'pick': 317,\n",
       " 'mitts': 2,\n",
       " 'located': 209,\n",
       " 'fenced': 6,\n",
       " 'area': 1044,\n",
       " 'let': 596,\n",
       " 'dogs': 157,\n",
       " 'run': 305,\n",
       " 'play': 186,\n",
       " 'sniff': 2,\n",
       " 'general': 95,\n",
       " 'manager': 288,\n",
       " 'scott': 26,\n",
       " 'petello': 1,\n",
       " 'egg': 275,\n",
       " 'detail': 48,\n",
       " 'assure': 11,\n",
       " 'albeit': 32,\n",
       " 'rare': 115,\n",
       " 'speak': 105,\n",
       " 'treat': 145,\n",
       " 'guy': 428,\n",
       " 'respect': 27,\n",
       " 'state': 82,\n",
       " 'surprised': 249,\n",
       " 'walk': 407,\n",
       " 'totally': 241,\n",
       " 'satisfied': 90,\n",
       " 'always': 2020,\n",
       " 'say': 1302,\n",
       " '.....': 191,\n",
       " 'mistakes': 10,\n",
       " 'inevitable': 3,\n",
       " 'recover': 7,\n",
       " 'important': 98,\n",
       " '\"!!!': 1,\n",
       " 'thanks': 219,\n",
       " 'staff': 1496,\n",
       " 'customer': 450,\n",
       " 'life': 246,\n",
       " '!!': 706,\n",
       " '..........': 4,\n",
       " ':^)': 1,\n",
       " 'quiessence': 6,\n",
       " 'put': 517,\n",
       " 'beautiful': 293,\n",
       " 'full': 646,\n",
       " 'windows': 61,\n",
       " 'earthy': 11,\n",
       " 'wooden': 25,\n",
       " 'walls': 116,\n",
       " 'feeling': 217,\n",
       " 'warmth': 7,\n",
       " 'inside': 592,\n",
       " 'restaurant': 1750,\n",
       " 'perched': 4,\n",
       " 'middle': 169,\n",
       " 'farm': 70,\n",
       " 'seemed': 370,\n",
       " 'fairly': 164,\n",
       " 'even': 2014,\n",
       " 'tuesday': 89,\n",
       " ';': 652,\n",
       " 'secured': 4,\n",
       " 'reservations': 73,\n",
       " 'couple': 517,\n",
       " 'days': 299,\n",
       " 'sampled': 40,\n",
       " 'sandwiches': 326,\n",
       " 'kitchen': 341,\n",
       " 'week': 476,\n",
       " 'impressed': 333,\n",
       " 'enough': 833,\n",
       " 'want': 1294,\n",
       " 'eat': 1175,\n",
       " 'crisp': 126,\n",
       " 'veggies': 197,\n",
       " 'disappoint': 58,\n",
       " 'ordered': 1705,\n",
       " 'salad': 1273,\n",
       " 'orange': 169,\n",
       " 'grapefruit': 15,\n",
       " 'slices': 152,\n",
       " 'crudites': 1,\n",
       " 'start': 322,\n",
       " 'know': 1613,\n",
       " 'much': 1754,\n",
       " 'raw': 81,\n",
       " 'radishes': 4,\n",
       " 'turnips': 1,\n",
       " 'tried': 762,\n",
       " 'pesto': 83,\n",
       " 'aioli': 21,\n",
       " 'sauces': 158,\n",
       " 'entrees': 209,\n",
       " 'lamb': 125,\n",
       " 'pork': 550,\n",
       " 'shoulder': 24,\n",
       " 'service': 3169,\n",
       " 'started': 386,\n",
       " 'trailed': 1,\n",
       " 'waiting': 379,\n",
       " 'long': 815,\n",
       " 'time': 3504,\n",
       " 'received': 169,\n",
       " 'finished': 152,\n",
       " '),': 816,\n",
       " 'bothered': 30,\n",
       " 'explain': 77,\n",
       " 'situation': 69,\n",
       " 'maitre': 4,\n",
       " 'apologized': 52,\n",
       " 'almost': 514,\n",
       " '45': 105,\n",
       " 'minutes': 797,\n",
       " 'later': 312,\n",
       " 'apparently': 132,\n",
       " 'chef': 258,\n",
       " 'unhappy': 18,\n",
       " 'entree': 186,\n",
       " 'anew': 2,\n",
       " 'really': 3366,\n",
       " 'problem': 291,\n",
       " 'communicated': 3,\n",
       " 'troubles': 2,\n",
       " 'comped': 19,\n",
       " 'glass': 358,\n",
       " 'wine': 808,\n",
       " 'forgot': 108,\n",
       " 'bring': 411,\n",
       " 'requested': 40,\n",
       " 'offer': 350,\n",
       " 'echo': 21,\n",
       " 'lady': 171,\n",
       " 'whispered': 2,\n",
       " 'way': 1470,\n",
       " 'ask': 559,\n",
       " 'warm': 319,\n",
       " 'foccacia': 7,\n",
       " 'apple': 103,\n",
       " 'walnut': 29,\n",
       " 'pomegranate': 20,\n",
       " 'wonder': 86,\n",
       " 'honey': 133,\n",
       " 'butter': 267,\n",
       " 'yum': 216,\n",
       " 'solid': 143,\n",
       " 'quite': 653,\n",
       " 'live': 453,\n",
       " 'innovation': 2,\n",
       " 'freshness': 23,\n",
       " 'vegetables': 121,\n",
       " 'meat': 730,\n",
       " 'tough': 86,\n",
       " 'maybe': 657,\n",
       " 'vegetarian': 175,\n",
       " '?': 2172,\n",
       " 'dessert': 428,\n",
       " 'gingerbread': 4,\n",
       " 'pear': 31,\n",
       " 'cake': 279,\n",
       " 'yet': 440,\n",
       " 'another': 967,\n",
       " 'winner': 41,\n",
       " 'tad': 59,\n",
       " 'inspired': 33,\n",
       " 'spotty': 11,\n",
       " 'definitely': 1146,\n",
       " 'warranted': 3,\n",
       " 'five': 265,\n",
       " 'stars': 862,\n",
       " 'return': 356,\n",
       " '75': 65,\n",
       " '$': 2113,\n",
       " 'tasting': 174,\n",
       " 'bill': 261,\n",
       " '100': 159,\n",
       " 'two': 1355,\n",
       " 'including': 211,\n",
       " 'tip': 245,\n",
       " 'drinks': 858,\n",
       " 'drop': 100,\n",
       " 'drive': 411,\n",
       " 'ate': 333,\n",
       " 'next': 1069,\n",
       " 'day': 1223,\n",
       " 'cute': 275,\n",
       " 'little': 2221,\n",
       " 'green': 480,\n",
       " 'building': 151,\n",
       " 'may': 514,\n",
       " 'gone': 228,\n",
       " 'competely': 1,\n",
       " 'unoticed': 1,\n",
       " 'driving': 105,\n",
       " 'palm': 8,\n",
       " 'rd': 37,\n",
       " 'avoid': 112,\n",
       " 'construction': 21,\n",
       " 'turn': 114,\n",
       " 'onto': 67,\n",
       " '16th': 12,\n",
       " 'street': 368,\n",
       " 'grand': 57,\n",
       " 'opening': 115,\n",
       " 'sign': 179,\n",
       " 'caught': 58,\n",
       " 'eye': 143,\n",
       " 'yelping': 9,\n",
       " 'soul': 32,\n",
       " 'leaped': 1,\n",
       " 'joy': 30,\n",
       " 'new': 1230,\n",
       " 'desolate': 3,\n",
       " 'opened': 198,\n",
       " 'easy': 322,\n",
       " 'decor': 412,\n",
       " 'smell': 128,\n",
       " 'cleanliness': 34,\n",
       " 'dinner': 990,\n",
       " 'loved': 520,\n",
       " 'seeing': 136,\n",
       " 'variety': 289,\n",
       " 'poblano': 13,\n",
       " 'peppers': 140,\n",
       " 'mole': 31,\n",
       " 'mahi': 40,\n",
       " 'mushrooms': 143,\n",
       " 'wrapped': 75,\n",
       " 'banana': 104,\n",
       " 'leaves': 69,\n",
       " 'difficult': 103,\n",
       " 'choose': 267,\n",
       " 'far': 585,\n",
       " 'la': 164,\n",
       " 'condesa': 10,\n",
       " 'shrimp': 455,\n",
       " 'burro': 27,\n",
       " 'baja': 26,\n",
       " 'sur': 7,\n",
       " 'dogfish': 4,\n",
       " 'shark': 1,\n",
       " 'taco': 249,\n",
       " 'meals': 194,\n",
       " 'stole': 11,\n",
       " 'flavor': 679,\n",
       " 'snagged': 8,\n",
       " 'bites': 65,\n",
       " 'hubbys': 2,\n",
       " 'burros': 7,\n",
       " 'mmmm': 33,\n",
       " 'delight': 51,\n",
       " 'salsa': 398,\n",
       " 'bar': 1602,\n",
       " 'endless': 23,\n",
       " 'stocked': 55,\n",
       " 'excited': 189,\n",
       " 'strawberry': 90,\n",
       " 'hot': 865,\n",
       " 'fact': 396,\n",
       " 'big': 931,\n",
       " 'wimp': 5,\n",
       " 'horchata': 34,\n",
       " 'handmade': 19,\n",
       " 'throw': 82,\n",
       " 'pecans': 18,\n",
       " 'fruit': 162,\n",
       " 'yummy': 401,\n",
       " 'bonus': 96,\n",
       " 'win': 50,\n",
       " 'art': 155,\n",
       " 'sho': 3,\n",
       " 'sucker': 35,\n",
       " 'mexican': 528,\n",
       " 'folk': 13,\n",
       " 'frida': 3,\n",
       " 'kahlo': 3,\n",
       " 'oprah': 4,\n",
       " 'painting': 18,\n",
       " 'diego': 30,\n",
       " 'hanging': 84,\n",
       " 'paintings': 17,\n",
       " 'artist': 25,\n",
       " 'luckily': 79,\n",
       " 'travel': 69,\n",
       " 'make': 1520,\n",
       " 'connecting': 10,\n",
       " 'flight': 94,\n",
       " 'thank': 199,\n",
       " 'phoenix': 961,\n",
       " 'brief': 13,\n",
       " 'layover': 7,\n",
       " 'employees': 222,\n",
       " 'kind': 691,\n",
       " 'hopefully': 67,\n",
       " 'grace': 29,\n",
       " 'presence': 17,\n",
       " 'longer': 175,\n",
       " 'come': 1244,\n",
       " 'happy': 1141,\n",
       " 'hour': 974,\n",
       " 'sake': 76,\n",
       " 'bombers': 9,\n",
       " '3': 1187,\n",
       " 'atmosphere': 799,\n",
       " 'incredibly': 107,\n",
       " 'nice': 2217,\n",
       " 'right': 1371,\n",
       " 'needs': 207,\n",
       " 'thing': 987,\n",
       " 'spot': 574,\n",
       " 'gets': 311,\n",
       " 'especially': 449,\n",
       " 'plan': 140,\n",
       " 'wish': 337,\n",
       " 'apollo': 3,\n",
       " 'beach': 37,\n",
       " 'brandon': 2,\n",
       " 'nobuo': 10,\n",
       " 'shows': 78,\n",
       " 'unique': 255,\n",
       " 'talents': 3,\n",
       " 'carefully': 12,\n",
       " 'crafted': 12,\n",
       " 'features': 32,\n",
       " 'belly': 58,\n",
       " 'buns': 33,\n",
       " 'stout': 23,\n",
       " 'oldish': 1,\n",
       " 'man': 263,\n",
       " 'owns': 15,\n",
       " 'store': 822,\n",
       " 'sweet': 840,\n",
       " 'perhaps': 165,\n",
       " 'sweeter': 12,\n",
       " 'cookies': 126,\n",
       " 'ice': 503,\n",
       " 'cream': 614,\n",
       " 'lowdown': 3,\n",
       " 'giant': 76,\n",
       " 'cookie': 124,\n",
       " 'super': 653,\n",
       " 'cheap': 417,\n",
       " 'permutations': 1,\n",
       " 'basically': 131,\n",
       " 'snickerdoodle': 9,\n",
       " 'marvelous': 9,\n",
       " 'vietnamese': 115,\n",
       " 'sandwich': 882,\n",
       " 'shoppe': 4,\n",
       " 'baguettes': 3,\n",
       " 'oven': 85,\n",
       " 'choices': 213,\n",
       " 'modest': 20,\n",
       " 'goods': 56,\n",
       " 'along': 264,\n",
       " 'rolls': 319,\n",
       " 'around': 1132,\n",
       " 'cash': 141,\n",
       " 'atm': 24,\n",
       " 'card': 214,\n",
       " 'credit': 103,\n",
       " 'cards': 63,\n",
       " 'accepted': 17,\n",
       " 'premises': 12,\n",
       " 'limited': 122,\n",
       " 'going': 1423,\n",
       " 'bbq': 372,\n",
       " 'chicken': 1746,\n",
       " 'last': 969,\n",
       " 'tomato': 252,\n",
       " 'basil': 132,\n",
       " 'soup': 545,\n",
       " 'every': 1065,\n",
       " 'rate': 91,\n",
       " 'jason': 23,\n",
       " 'deli': 107,\n",
       " '4': 942,\n",
       " 'tattoo': 10,\n",
       " 'shop': 467,\n",
       " 'space': 233,\n",
       " 'multiple': 76,\n",
       " 'artists': 23,\n",
       " 'books': 90,\n",
       " 'work': 763,\n",
       " 'available': 229,\n",
       " 'look': 631,\n",
       " 'though': 1055,\n",
       " 'decide': 82,\n",
       " 'style': 446,\n",
       " 'mirrors': 14,\n",
       " 'looking': 801,\n",
       " 'chose': 121,\n",
       " 'jet': 6,\n",
       " 'cover': 90,\n",
       " 'worked': 145,\n",
       " 'design': 67,\n",
       " 'ideas': 19,\n",
       " 'communication': 9,\n",
       " 'flowed': 3,\n",
       " 'personable': 39,\n",
       " 'friendly': 1527,\n",
       " 'keeps': 61,\n",
       " 'conversation': 113,\n",
       " 'working': 274,\n",
       " 'dick': 18,\n",
       " 'read': 249,\n",
       " 'starts': 42,\n",
       " 'continues': 23,\n",
       " 'done': 436,\n",
       " ').': 1498,\n",
       " 'professional': 133,\n",
       " 'informative': 28,\n",
       " 'combines': 5,\n",
       " 'talent': 15,\n",
       " 'craft': 47,\n",
       " 'weeks': 171,\n",
       " 'irish': 56,\n",
       " 'bars': 136,\n",
       " 'town': 635,\n",
       " 'found': 703,\n",
       " 'ambience': 68,\n",
       " 'outstanding': 135,\n",
       " 'went': 1516,\n",
       " 'wednesday': 61,\n",
       " 'night': 1485,\n",
       " 'hopes': 35,\n",
       " 'finding': 90,\n",
       " 'folks': 142,\n",
       " 'exactly': 188,\n",
       " 'happened': 131,\n",
       " 'marcy': 1,\n",
       " 'bartender': 205,\n",
       " 'helpful': 377,\n",
       " 'introduced': 41,\n",
       " 'whole': 490,\n",
       " 'bunch': 128,\n",
       " 'rueben': 3,\n",
       " 'large': 645,\n",
       " 'side': 960,\n",
       " 'lettuce': 195,\n",
       " 'iceburg': 6,\n",
       " 'bacon': 378,\n",
       " '&': 1126,\n",
       " 'toppings': 211,\n",
       " 'helped': 119,\n",
       " 'drank': 31,\n",
       " 'cold': 287,\n",
       " 'smithwick': 3,\n",
       " 'tap': 125,\n",
       " 'followed': 49,\n",
       " 'vodka': 64,\n",
       " 'tonics': 1,\n",
       " 'tab': 33,\n",
       " '25': 169,\n",
       " 'basketball': 21,\n",
       " 'game': 287,\n",
       " 'first': 1636,\n",
       " 'jukebox': 21,\n",
       " 'playing': 165,\n",
       " 'fabulous': 171,\n",
       " 'mix': 170,\n",
       " 'oldies': 5,\n",
       " '10pm': 21,\n",
       " 'band': 95,\n",
       " 'played': 67,\n",
       " 'catch': 75,\n",
       " 'performance': 22,\n",
       " 'notice': 95,\n",
       " 'singer': 8,\n",
       " 'voice': 33,\n",
       " 'since': 1150,\n",
       " 'typically': 86,\n",
       " 'care': 450,\n",
       " 'female': 23,\n",
       " 'vocalists': 1,\n",
       " 'spoke': 40,\n",
       " 'welcoming': 80,\n",
       " 'see': 1175,\n",
       " 'forward': 175,\n",
       " 'worth': 790,\n",
       " '21': 41,\n",
       " 'guys': 278,\n",
       " 'grandma': 32,\n",
       " 'died': 15,\n",
       " 'tell': 417,\n",
       " 'mad': 29,\n",
       " 'experience': 1157,\n",
       " 'could': 1564,\n",
       " 'cared': 17,\n",
       " 'less': 462,\n",
       " 'sat': 375,\n",
       " 'hmm': 26,\n",
       " 'saying': 194,\n",
       " 'x': 25,\n",
       " '23': 14,\n",
       " '$\".': 1,\n",
       " 'wow': 233,\n",
       " 'told': 615,\n",
       " 'left': 553,\n",
       " 'hungry': 199,\n",
       " 'unsatisfied': 7,\n",
       " 'owner': 367,\n",
       " 'teach': 17,\n",
       " 'value': 180,\n",
       " 'upselling': 3,\n",
       " 'telling': 49,\n",
       " 'specials': 242,\n",
       " 'affect': 7,\n",
       " 'customers': 251,\n",
       " 'negatively': 2,\n",
       " 'salads': 230,\n",
       " 'severely': 13,\n",
       " 'overpriced': 128,\n",
       " 'unless': 189,\n",
       " 'desperate': 10,\n",
       " 'afternoon': 194,\n",
       " 'empty': 193,\n",
       " 'brunch': 158,\n",
       " 'mimosas': 21,\n",
       " 'mood': 144,\n",
       " 'lunch': 1272,\n",
       " 'except': 163,\n",
       " 'high': 556,\n",
       " 'ball': 74,\n",
       " 'sized': 133,\n",
       " 'boo': 20,\n",
       " 'yay': 56,\n",
       " 'hubby': 116,\n",
       " 'remembered': 63,\n",
       " 'arrogant': 13,\n",
       " 'bastard': 6,\n",
       " '22': 29,\n",
       " 'oz': 72,\n",
       " 'bottle': 203,\n",
       " 'hey': 111,\n",
       " 'fair': 170,\n",
       " 'wings': 273,\n",
       " 'bit': 1059,\n",
       " 'hesitant': 23,\n",
       " 'informed': 60,\n",
       " 'seasoned': 132,\n",
       " 'sauced': 12,\n",
       " 'crispy': 239,\n",
       " 'asked': 700,\n",
       " 'cooks': 35,\n",
       " 'visibly': 5,\n",
       " 'non': 198,\n",
       " 'traditional': 144,\n",
       " 'actually': 720,\n",
       " 'damn': 146,\n",
       " 'seasoning': 50,\n",
       " 'spicy': 547,\n",
       " 'salty': 143,\n",
       " 'hint': 56,\n",
       " 'tang': 9,\n",
       " 'kick': 122,\n",
       " 'frank': 39,\n",
       " 'cut': 254,\n",
       " 'otherwise': 115,\n",
       " 'tilapia': 16,\n",
       " 'disappointed': 421,\n",
       " 'fish': 474,\n",
       " 'dry': 284,\n",
       " 'uninspired': 11,\n",
       " 'greens': 86,\n",
       " 'underneath': 15,\n",
       " 'overdressed': 2,\n",
       " 'wilted': 12,\n",
       " 'picked': 138,\n",
       " 'almonds': 14,\n",
       " 'mandarin': 13,\n",
       " 'oranges': 11,\n",
       " 'leave': 274,\n",
       " 'mush': 10,\n",
       " 'hiding': 14,\n",
       " 'anxiously': 4,\n",
       " 'awaiting': 6,\n",
       " 'trip': 293,\n",
       " 'okay': 370,\n",
       " 'grew': 45,\n",
       " 'shopping': 252,\n",
       " 'los': 68,\n",
       " 'gatos': 1,\n",
       " 'oakville': 3,\n",
       " 'shock': 13,\n",
       " 'saw': 336,\n",
       " 'world': 209,\n",
       " 'stuff': 449,\n",
       " 'cheese': 1378,\n",
       " 'arizona': 313,\n",
       " 'dean': 5,\n",
       " 'deluca': 1,\n",
       " 'met': 134,\n",
       " 'yesterday': 88,\n",
       " 'water': 523,\n",
       " 'feature': 38,\n",
       " 'patio': 528,\n",
       " 'walking': 229,\n",
       " 'warmer': 17,\n",
       " 'dark': 198,\n",
       " 'guess': 354,\n",
       " 'bats': 6,\n",
       " 'reminded': 71,\n",
       " 'dusk': 4,\n",
       " 'till': 48,\n",
       " 'dawn': 4,\n",
       " '\".': 282,\n",
       " 'hope': 220,\n",
       " 'hoping': 85,\n",
       " 'chips': 478,\n",
       " 'serve': 328,\n",
       " 'offered': 223,\n",
       " 'ones': 232,\n",
       " 'declined': 18,\n",
       " 'complimentary': 75,\n",
       " 'anyhow': 24,\n",
       " 'snapper': 6,\n",
       " 'tacos': 463,\n",
       " 'pics': 26,\n",
       " 'smothered': 27,\n",
       " '15': 326,\n",
       " 'tbsp': 1,\n",
       " 'beans': 344,\n",
       " 'teaspoon': 2,\n",
       " 'sour': 143,\n",
       " 'lime': 68,\n",
       " 'lemon': 137,\n",
       " 'garnish': 16,\n",
       " 'anything': 661,\n",
       " 'chile': 107,\n",
       " 'stew': 47,\n",
       " 'mac': 192,\n",
       " 'knew': 226,\n",
       " 'tasted': 459,\n",
       " 'macaroni': 36,\n",
       " 'gotten': 113,\n",
       " 'review': 610,\n",
       " 'written': 42,\n",
       " 'contact': 52,\n",
       " 'ago': 334,\n",
       " 'trouble': 49,\n",
       " 'redeeming': 13,\n",
       " 'groupons': 4,\n",
       " 'website': 110,\n",
       " 'called': 390,\n",
       " 'rep': 7,\n",
       " 'cheerfully': 7,\n",
       " 'booked': 36,\n",
       " 'four': 329,\n",
       " 'separate': 67,\n",
       " 'flights': 22,\n",
       " 'patiently': 13,\n",
       " 'manually': 3,\n",
       " 'entering': 41,\n",
       " 'groupon': 91,\n",
       " 'info': 17,\n",
       " 'think': 1552,\n",
       " 'acquisition': 1,\n",
       " 'republic': 22,\n",
       " 'overall': 549,\n",
       " 'still': 1212,\n",
       " 'tends': 20,\n",
       " 'suck': 37,\n",
       " 'ascent': 2,\n",
       " 'club': 208,\n",
       " 'budget': 53,\n",
       " 'centric': 4,\n",
       " 'airline': 12,\n",
       " 'fly': 51,\n",
       " 'southwest': 45,\n",
       " 'usair': 1,\n",
       " 'comparison': 35,\n",
       " 'dvap': 1,\n",
       " '....': 842,\n",
       " 'least': 536,\n",
       " 'neat': 65,\n",
       " 'alot': 53,\n",
       " 'history': 42,\n",
       " 'appears': 43,\n",
       " 'family': 535,\n",
       " 'dennys': 6,\n",
       " 'mimi': 16,\n",
       " 'u': 56,\n",
       " 'ribs': 176,\n",
       " 'lasagna': 24,\n",
       " 'loaf': 17,\n",
       " 'cat': 34,\n",
       " 'mashed': 100,\n",
       " 'diced': 13,\n",
       " 'potatoes': 282,\n",
       " 'stuffing': 27,\n",
       " 'homemade': 170,\n",
       " 'pie': 172,\n",
       " 'etc': 357,\n",
       " 'missing': 107,\n",
       " 'drift': 10,\n",
       " 'prime': 66,\n",
       " 'rib': 84,\n",
       " 'might': 406,\n",
       " 'call': 386,\n",
       " 'serving': 163,\n",
       " 'reviewed': 32,\n",
       " 'keep': 387,\n",
       " '=)': 26,\n",
       " 'garlic': 243,\n",
       " 'knots': 9,\n",
       " 'favorite': 887,\n",
       " 'course': 509,\n",
       " 'n': 174,\n",
       " 'check': 708,\n",
       " 'car': 359,\n",
       " 'wanna': 34,\n",
       " 'buy': 314,\n",
       " '1': 831,\n",
       " 'wrong': 370,\n",
       " 'move': 115,\n",
       " 'biggest': 83,\n",
       " 'mistake': 80,\n",
       " 'ur': 3,\n",
       " 'girlfriend': 105,\n",
       " 'oil': 197,\n",
       " 'ripped': 21,\n",
       " 'lying': 9,\n",
       " 'without': 550,\n",
       " 'fixing': 15,\n",
       " 'accident': 18,\n",
       " 'brand': 86,\n",
       " 'tires': 38,\n",
       " 'timing': 16,\n",
       " 'belt': 24,\n",
       " 'brake': 4,\n",
       " 'pads': 5,\n",
       " 'worst': 158,\n",
       " 'changed': 119,\n",
       " 'months': 214,\n",
       " 'trashy': 5,\n",
       " 'dealer': 6,\n",
       " 'somewhere': 147,\n",
       " 'coming': 419,\n",
       " 'ages': 26,\n",
       " 'favorites': 125,\n",
       " 'elsa': 2,\n",
       " 'burgers': 288,\n",
       " 'dragon': 19,\n",
       " 'china': 64,\n",
       " 'pepper': 126,\n",
       " 'fun': 625,\n",
       " 'display': 37,\n",
       " 'abstract': 2,\n",
       " 'cool': 639,\n",
       " 'ole': 9,\n",
       " 'fashion': 78,\n",
       " 'real': 419,\n",
       " 'vibe': 156,\n",
       " 'sorta': 18,\n",
       " 'reminds': 74,\n",
       " 'sugar': 129,\n",
       " 'bowl': 309,\n",
       " 'smaller': 109,\n",
       " 'easier': 44,\n",
       " 'parking': 564,\n",
       " 'ginormous': 11,\n",
       " 'matter': 142,\n",
       " 'basic': 88,\n",
       " 'insisted': 29,\n",
       " 'sharing': 52,\n",
       " 'patte': 1,\n",
       " 'scoops': 18,\n",
       " 'brownie': 55,\n",
       " 'topped': 110,\n",
       " 'whip': 13,\n",
       " 'caramel': 65,\n",
       " 'scoop': 39,\n",
       " 'taste': 734,\n",
       " 'brittle': 11,\n",
       " 'tons': 149,\n",
       " 'different': 770,\n",
       " 'flavors': 322,\n",
       " 'sound': 125,\n",
       " 'delish': 108,\n",
       " 'need': 786,\n",
       " 'rush': 81,\n",
       " 'boss': 21,\n",
       " 'healthy': 179,\n",
       " 'eating': 507,\n",
       " 'recommended': 170,\n",
       " 'highly': 340,\n",
       " 'skeptical': 21,\n",
       " 'friends': 707,\n",
       " 'convert': 8,\n",
       " 'believers': 2,\n",
       " 'dumplings': 37,\n",
       " 'onion': 241,\n",
       " 'tart': 59,\n",
       " 'starters': 27,\n",
       " 'shirataki': 1,\n",
       " 'noodles': 202,\n",
       " 'kale': 25,\n",
       " 'aid': 33,\n",
       " 'dishes': 521,\n",
       " 'times': 1088,\n",
       " 'never': 1541,\n",
       " 'yoga': 48,\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_words_wo_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var = \"chandr mouli rajesh rree chandra chandra mouli mouli rajesh rajesh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud ,STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ac7f4960ab05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(stopwords=[]).generate(str(var.tolist()))\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(200,100))\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,  valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB  for L1, Count Vectors:  0.4736\n",
      "NB  for L1, WordLevel TF-IDF:  0.508\n",
      "NB  for L1, N-Gram Vectors:  0.4512\n",
      "NB for L1, CharLevel Vectors:  0.4028\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# Naive Bayes on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(\"NB  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xtest_count, y_test)\n",
    "print(\"NB  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\n",
    "print(\"NB  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\n",
    "print(\"NB for L1, CharLevel Vectors: \", accuracy_L1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  for L1, Count Vectors:  0.5164\n",
      "LR  for L1, WordLevel TF-IDF:  0.4824\n",
      "LR  for L1, N-Gram Vectors:  0.4988\n",
      "LR for L1, CharLevel Vectors:  0.4324\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# Naive Bayes on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(LogisticRegression(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(\"LR  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), xtrain_count, y_train, xtest_count, y_test)\n",
    "print(\"LR  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\n",
    "print(\"LR  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(LogisticRegression(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\n",
    "print(\"LR for L1, CharLevel Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR  for L1, Count Vectors:  0.4976\n",
      "LR  for L1, WordLevel TF-IDF:  0.4856\n",
      "LR  for L1, N-Gram Vectors:  0.4976\n",
      "LR for L1, CharLevel Vectors:  0.4608\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# Naive Bayes on Count Vectors and TF-IDF\n",
    "accuracy_L1 = train_model(svm.LinearSVC(), X_train_tfidf, y_train, X_test_tfidf, y_test)\n",
    "print(\"LR  for L1, Count Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(svm.LinearSVC(), xtrain_count, y_train, xtest_count, y_test)\n",
    "print(\"LR  for L1, WordLevel TF-IDF: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(svm.LinearSVC(), xtrain_tfidf_ngram, y_train, xtest_tfidf_ngram, y_test)\n",
    "print(\"LR  for L1, N-Gram Vectors: \", accuracy_L1)\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy_L1 = train_model(svm.LinearSVC(), xtrain_tfidf_ngram_chars, y_train, xtest_tfidf_ngram_chars, y_test)\n",
    "print(\"LR for L1, CharLevel Vectors: \", accuracy_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** boolean, True by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.82384531  0.28730789  0.20464882]\n",
      " [ 0.82384531  1.          0.16511247  0.1679379 ]\n",
      " [ 0.28730789  0.16511247  1.          0.89268279]\n",
      " [ 0.20464882  0.1679379   0.89268279  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Calculate tf-idf:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform([\"New Year's Eve in New York\",\n",
    "                            \"New Year's Eve in London\",\n",
    "                            \"York is closer to London than to New York\",\n",
    "                            \"London is closer to Bucharest than to New York\"])\n",
    "\n",
    "#Calculate cosine similarity:\n",
    "cosine=(tfidf * tfidf.T).A\n",
    "print(cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting the star rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print 'Features: ', X_train_dtm.shape[1]\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print 'Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of stop words\n",
    "print vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all 100 features\n",
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Introduction to TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the first review\n",
    "print(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(yelp.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(review.ngrams(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem each word\n",
    "print [stemmer.stem(word) for word in review.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assume every word is a noun\n",
    "print [word.lemmatize() for word in review.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assume every word is a verb\n",
    "print [word.lemmatize(pos='v') for word in review.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last 50 features\n",
    "print vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print 'TOP SCORING WORDS:'\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print word\n",
    "    \n",
    "    # print 5 random words\n",
    "    print '\\n' + 'RANDOM WORDS:'\n",
    "    random_words = np.random.choice(word_scores.keys(), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print word\n",
    "    \n",
    "    # print the review\n",
    "    print '\\n' + review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# negative sentiment in a 5-star review\n",
    "print yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# positive sentiment in a 1-star review\n",
    "print yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "#yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp[feature_cols]\n",
    "y = yelp.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 25797)\n",
      "(2500, 25797)\n"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 4)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 25801)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 25801)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for testing set\n",
    "extra = sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4592\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4672\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=\"this is bcz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"this is bc\")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(s).correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field\n",
    "- Understanding the basics broadens the types of data you can work with\n",
    "- Simple techniques go a long way\n",
    "- Use scikit-learn for NLP whenever possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
